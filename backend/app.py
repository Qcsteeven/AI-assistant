# backend/app.py
from typing import List, Optional, Tuple
import faiss
import numpy as np
from docx import Document
from openai import OpenAI
import os

class VectorBackend:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.__init__()
        return cls._instance
    
    def __init__(self):
        if not hasattr(self, 'is_initialized'):  # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            self.index: Optional[faiss.Index] = None
            self.embeddings: List[List[float]] = []
            self.chunks: List[str] = []
            self.is_initialized = True

    def load_first_docx(self, chunk_size: int = 500) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–µ—Ä–≤—ã–π .docx —Ñ–∞–π–ª –∏–∑ –ø–∞–ø–∫–∏ data"""
        data_dir = "/app/data"  # –ü—É—Ç—å –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
        if not os.path.exists(data_dir):
            raise FileNotFoundError(f"–ü–∞–ø–∫–∞ {data_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

        # –ò—â–µ–º –ø–µ—Ä–≤—ã–π .docx —Ñ–∞–π–ª
        docx_files = [f for f in os.listdir(data_dir) if f.lower().endswith('.docx')]
        if not docx_files:
            raise FileNotFoundError(f"–í –ø–∞–ø–∫–µ {data_dir} –Ω–µ—Ç .docx —Ñ–∞–π–ª–æ–≤")

        first_doc = os.path.join(data_dir, docx_files[0])
        return self.load_docx_chunks(first_doc, chunk_size)

    def load_docx_chunks(self, path: str, chunk_size: int = 500) -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        doc = Document(path)
        chunks = []
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–±–∑–∞—Ü–µ–≤
        for para in doc.paragraphs:
            if text := para.text.strip():
                chunks.append(text)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü
        for table_idx, table in enumerate(doc.tables, 1):
            for row_idx, row in enumerate(table.rows, 1):
                row_text = " | ".join(cell.text.strip() for cell in row.cells if cell.text.strip())
                if row_text:
                    chunks.append(f"–¢–∞–±–ª–∏—Ü–∞ {table_idx}, —Å—Ç—Ä–æ–∫–∞ {row_idx}: {row_text}")

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        final_chunks = []
        for chunk in chunks:
            words = chunk.split()
            if len(words) <= chunk_size:
                final_chunks.append(chunk)
            else:
                final_chunks.extend([
                    " ".join(words[i:i + chunk_size]) 
                    for i in range(0, len(words), chunk_size)
                ])
        
        self.chunks = final_chunks
        return final_chunks

    def get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ OpenAI"""
        response = self.client.embeddings.create(
            input=[text],
            model="text-embedding-ada-002"
        )
        return response.data[0].embedding

    def build_faiss_index(self, chunks: List[str]) -> Tuple[faiss.Index, List[List[float]]]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞"""
        self.embeddings = [self.get_embedding(chunk) for chunk in chunks]
        dim = len(self.embeddings[0])
        self.index = faiss.IndexFlatL2(dim)
        self.index.add(np.array(self.embeddings).astype("float32"))
        return self.index, self.embeddings

    def search_similar_chunks(self, query: str, k: int = 3) -> List[str]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —á–∞–Ω–∫–æ–≤"""
        if not self.index:
            raise ValueError("–ò–Ω–¥–µ–∫—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ build_faiss_index()")
            
        query_vec = np.array([self.get_embedding(query)]).astype("float32")
        _, indices = self.index.search(query_vec, k)
        return [self.chunks[i] for i in indices[0]]

    def chat(self, memory_limit: int = 5):
        """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Ç —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º"""
        if not self.index:
            raise ValueError("–ò–Ω–¥–µ–∫—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        print("üí¨ –ß–∞—Ç —Å –ò–ò. –ù–∞–ø–∏—à–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞.")
        history = []
        
        while True:
            query = input("\n–¢—ã: ")
            if query.lower() == "exit":
                break
                
            top_chunks = self.search_similar_chunks(query)
            context = "\n---\n".join(top_chunks)
            
            history_context = "\n".join(
                f"–í–æ–ø—Ä–æ—Å: {q}\n–û—Ç–≤–µ—Ç: {a}" 
                for q, a in history[-memory_limit:]
            )
            
            prompt = (
                "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É.\n"
                f"–ò—Å—Ç–æ—Ä–∏—è:\n{history_context}\n\n"
                f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n"
                f"–í–æ–ø—Ä–æ—Å: {query}\n–û—Ç–≤–µ—Ç:"
            )
            
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            
            answer = response.choices[0].message.content.strip()
            print("\nGPT:", answer)
            history.append((query, answer))